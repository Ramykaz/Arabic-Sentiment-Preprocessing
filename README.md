# Arabic Sentiment Preprocessing  

This repository contains Python scripts for **preprocessing Arabic sentiment datasets**, focusing on text cleaning, tokenization, feature extraction, and retweet network analysis. The goal is to enhance data quality for **sentiment classification models**, particularly **Convolutional Neural Networks (CNNs)** trained on word embeddings (GloVe, Skip-Gram, CBOW).  

This project was part of my **research internship under Prof. Hala Mulki** at **OrtadoÄŸu AraÅŸtÄ±rmalarÄ± Merkezi - ORSAM**. My primary contribution was **data preprocessing and analysis**, ensuring the dataset was optimized for deep learning-based sentiment classification.  

ğŸ“„ **Read the research paper of my Supervisor for Context:** [docs/Empirical-Evaluation-of-Word-Representations-on-Arabic-Sentiment-Analysis.pdf](docs/Empirical-Evaluation-of-Word-Representations-on-Arabic-Sentiment-Analysis.pdf) 

I have written paper on the tasks I did and how preprocessing ensure dataset was optimizsed for the deep-learning based sentimnet classification, you can find that at:  [docs/Reasearch_Paper.pdf](docs/Research_paperpdf) 

---

## ğŸ“‚ Project Structure  

```
Arabic-Sentiment-Preprocessing/
â”‚â”€â”€ data/                   # Raw and processed datasets
â”‚   â”œâ”€â”€ raw_data.csv        # Original dataset
â”‚   â”œâ”€â”€ cleaned_data.csv    # Preprocessed dataset
â”‚â”€â”€ notebooks/              # Jupyter Notebooks for data analysis
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb  
â”‚â”€â”€ scripts/                # Python preprocessing scripts
â”‚   â”œâ”€â”€ text_cleaning.py    # Removes stopwords, special symbols, etc.
â”‚   â”œâ”€â”€ tokenization.py     # Tokenization & normalization for Arabic text
â”‚   â”œâ”€â”€ ngram_analysis.py   # Generates n-grams & word clouds
â”‚   â”œâ”€â”€ network_analysis.py # Retweet network visualization
â”‚â”€â”€ docs/                   # Research paper & documentation
â”‚   â”œâ”€â”€ Arabic_Sentiment_Preprocessing.pdf  
â”‚â”€â”€ README.md               # Project overview
â”‚â”€â”€ requirements.txt        # Dependencies
â”‚â”€â”€ .gitignore              # Ignore unnecessary files
```

---

## ğŸš€ Installation & Setup  

### 1ï¸âƒ£ Clone the repository  
```sh
git clone https://github.com/yourusername/Arabic-Sentiment-Preprocessing.git
cd Arabic-Sentiment-Preprocessing
```

### 2ï¸âƒ£ Install dependencies  
```sh
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run preprocessing scripts  

#### **1. Text Cleaning**  
```sh
python scripts/text_cleaning.py
```
Removes stopwords, punctuation, hashtags, and normalizes text.  

#### **2. Tokenization & Normalization**  
```sh
python scripts/tokenization.py
```
Tokenizes Arabic text, removes diacritics, and normalizes script variations.  

#### **3. N-gram Analysis & Word Cloud**  
```sh
python scripts/ngram_analysis.py
```
Extracts top unigrams/bigrams and generates word cloud visualization.  

#### **4. Retweet Network Analysis**  
```sh
python scripts/network_analysis.py
```
Builds and visualizes a retweet network graph for sentiment clustering.  

---

## ğŸ“Š Exploratory Data Analysis  

For **data visualization and insights**, open the Jupyter Notebook:  

```sh
jupyter notebook notebooks/exploratory_analysis.ipynb
```
Includes:
- Sentiment distribution graphs  
- Text length analysis  
- Word cloud visualization  

---


## ğŸ›  Dependencies  

Ensure you have the following Python libraries installed:  

```txt
pandas
numpy
nltk
matplotlib
seaborn
wordcloud
networkx
farasa
```

To install all dependencies:  
```sh
pip install -r requirements.txt
```

---
 

If you find this repository useful, consider **starring** â­ it on GitHub!  

---


